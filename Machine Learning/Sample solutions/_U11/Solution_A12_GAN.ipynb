{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN Solution.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r43hj6t5Vr4V"
      },
      "source": [
        "The following code is from this source with a only minor changes: https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypx0FAhePdvE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a788c946-b886-4909-c281-0496fa30c4a4"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "      \n",
        "n_epochs = 50         #number of epochs of training\n",
        "batch_size = 64       #size of the batches\n",
        "lr = 0.0002           #adam: learning rate\n",
        "b1 = 0.5              #adam: decay of first order momentum of gradient\n",
        "b2 = 0.999            #adam: decay of second order momentum of gradient\n",
        "n_cpu = 8,            #number of cpu threads to use during batch generation\n",
        "latent_dim = 100      #dimensionality of the latent space\n",
        "img_size = 28         #size of each image dimension\n",
        "channels = 1          #number of image channels\n",
        "sample_interval = 400 #interval between image samples\n",
        "\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "random_seed = 1\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"../../data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "        \n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "        if i%200 == 0:\n",
        "          print(\n",
        "              \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "              % (epoch+1, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        )\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], \"images/GAN-%d.png\" % batches_done, nrow=5, normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 1/50] [Batch 0/938] [D loss: 0.698792] [G loss: 0.676707]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 1/50] [Batch 200/938] [D loss: 0.636679] [G loss: 0.831796]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 1/50] [Batch 400/938] [D loss: 0.307583] [G loss: 1.599023]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 1/50] [Batch 600/938] [D loss: 0.238946] [G loss: 2.152054]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 1/50] [Batch 800/938] [D loss: 0.262854] [G loss: 1.122220]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 2/50] [Batch 0/938] [D loss: 0.336710] [G loss: 0.875174]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 2/50] [Batch 200/938] [D loss: 0.234831] [G loss: 1.355895]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 2/50] [Batch 400/938] [D loss: 0.346628] [G loss: 1.076400]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 2/50] [Batch 600/938] [D loss: 0.309195] [G loss: 1.169187]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 2/50] [Batch 800/938] [D loss: 0.179992] [G loss: 1.701983]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 3/50] [Batch 0/938] [D loss: 0.353346] [G loss: 3.489352]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 3/50] [Batch 200/938] [D loss: 0.236530] [G loss: 1.704377]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 3/50] [Batch 400/938] [D loss: 0.232723] [G loss: 1.727534]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 3/50] [Batch 600/938] [D loss: 0.455507] [G loss: 0.590288]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 3/50] [Batch 800/938] [D loss: 0.244373] [G loss: 1.306160]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 4/50] [Batch 0/938] [D loss: 0.223314] [G loss: 2.488347]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 4/50] [Batch 200/938] [D loss: 0.618495] [G loss: 4.826662]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 4/50] [Batch 400/938] [D loss: 0.183234] [G loss: 1.871837]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 4/50] [Batch 600/938] [D loss: 0.198277] [G loss: 1.649703]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 4/50] [Batch 800/938] [D loss: 0.209780] [G loss: 1.903610]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 5/50] [Batch 0/938] [D loss: 0.164143] [G loss: 1.899165]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 5/50] [Batch 200/938] [D loss: 0.220515] [G loss: 1.265305]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 5/50] [Batch 400/938] [D loss: 0.390559] [G loss: 3.395657]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 5/50] [Batch 600/938] [D loss: 0.534399] [G loss: 0.492928]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 5/50] [Batch 800/938] [D loss: 0.241913] [G loss: 2.675699]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 6/50] [Batch 0/938] [D loss: 0.371579] [G loss: 0.843953]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 6/50] [Batch 200/938] [D loss: 0.778277] [G loss: 5.075097]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 6/50] [Batch 400/938] [D loss: 0.220510] [G loss: 2.341405]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 6/50] [Batch 600/938] [D loss: 0.175542] [G loss: 2.033267]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 6/50] [Batch 800/938] [D loss: 0.343932] [G loss: 0.964120]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 7/50] [Batch 0/938] [D loss: 0.218856] [G loss: 1.655030]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 7/50] [Batch 200/938] [D loss: 0.219524] [G loss: 2.415221]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 7/50] [Batch 400/938] [D loss: 0.164615] [G loss: 2.014040]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 7/50] [Batch 600/938] [D loss: 0.260985] [G loss: 1.998971]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 7/50] [Batch 800/938] [D loss: 0.268741] [G loss: 3.333467]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 8/50] [Batch 0/938] [D loss: 0.234169] [G loss: 2.069173]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 8/50] [Batch 200/938] [D loss: 0.403891] [G loss: 0.796660]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 8/50] [Batch 400/938] [D loss: 0.375973] [G loss: 0.768464]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 8/50] [Batch 600/938] [D loss: 0.246621] [G loss: 2.841119]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 8/50] [Batch 800/938] [D loss: 0.160786] [G loss: 2.012579]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 9/50] [Batch 0/938] [D loss: 0.260641] [G loss: 2.882927]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 9/50] [Batch 200/938] [D loss: 0.307325] [G loss: 1.045777]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 9/50] [Batch 400/938] [D loss: 0.287862] [G loss: 2.956426]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 9/50] [Batch 600/938] [D loss: 0.149647] [G loss: 1.938889]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 9/50] [Batch 800/938] [D loss: 0.147419] [G loss: 2.529107]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 10/50] [Batch 0/938] [D loss: 0.305267] [G loss: 3.237388]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 10/50] [Batch 200/938] [D loss: 0.387062] [G loss: 2.883975]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 10/50] [Batch 400/938] [D loss: 0.413721] [G loss: 3.793721]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 10/50] [Batch 600/938] [D loss: 0.234323] [G loss: 1.646705]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 10/50] [Batch 800/938] [D loss: 0.158021] [G loss: 2.023010]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 11/50] [Batch 0/938] [D loss: 0.237577] [G loss: 1.459151]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 11/50] [Batch 200/938] [D loss: 0.404300] [G loss: 0.737088]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 11/50] [Batch 400/938] [D loss: 0.216740] [G loss: 2.251254]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 11/50] [Batch 600/938] [D loss: 0.239436] [G loss: 2.233573]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 11/50] [Batch 800/938] [D loss: 0.331033] [G loss: 1.028463]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 12/50] [Batch 0/938] [D loss: 0.306471] [G loss: 1.637883]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 12/50] [Batch 200/938] [D loss: 0.216164] [G loss: 2.164898]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 12/50] [Batch 400/938] [D loss: 0.234043] [G loss: 1.968166]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 12/50] [Batch 600/938] [D loss: 0.216747] [G loss: 1.553227]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 12/50] [Batch 800/938] [D loss: 0.246290] [G loss: 1.791847]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 13/50] [Batch 0/938] [D loss: 0.367336] [G loss: 1.068470]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 13/50] [Batch 200/938] [D loss: 0.207985] [G loss: 3.411318]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 13/50] [Batch 400/938] [D loss: 0.341524] [G loss: 1.721608]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 13/50] [Batch 600/938] [D loss: 0.308713] [G loss: 1.289582]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 13/50] [Batch 800/938] [D loss: 0.270987] [G loss: 2.119301]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 14/50] [Batch 0/938] [D loss: 0.287765] [G loss: 1.915322]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 14/50] [Batch 200/938] [D loss: 0.271797] [G loss: 2.616599]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 14/50] [Batch 400/938] [D loss: 0.220564] [G loss: 1.557515]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 14/50] [Batch 600/938] [D loss: 0.226899] [G loss: 2.631025]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 14/50] [Batch 800/938] [D loss: 0.197412] [G loss: 3.090082]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 15/50] [Batch 0/938] [D loss: 0.424294] [G loss: 4.131875]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 15/50] [Batch 200/938] [D loss: 0.215094] [G loss: 2.177188]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 15/50] [Batch 400/938] [D loss: 0.266052] [G loss: 1.589067]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 15/50] [Batch 600/938] [D loss: 1.168669] [G loss: 6.241895]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 15/50] [Batch 800/938] [D loss: 0.304936] [G loss: 1.089504]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 16/50] [Batch 0/938] [D loss: 0.368655] [G loss: 1.128230]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 16/50] [Batch 200/938] [D loss: 0.261682] [G loss: 1.771446]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 16/50] [Batch 400/938] [D loss: 0.333611] [G loss: 3.133898]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 16/50] [Batch 600/938] [D loss: 0.153267] [G loss: 1.945672]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 16/50] [Batch 800/938] [D loss: 0.329678] [G loss: 1.189847]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 17/50] [Batch 0/938] [D loss: 0.269811] [G loss: 2.451467]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 17/50] [Batch 200/938] [D loss: 0.292312] [G loss: 1.816007]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 17/50] [Batch 400/938] [D loss: 0.192507] [G loss: 2.068877]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 17/50] [Batch 600/938] [D loss: 0.153101] [G loss: 2.008924]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 17/50] [Batch 800/938] [D loss: 0.228079] [G loss: 2.704167]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 18/50] [Batch 0/938] [D loss: 0.267300] [G loss: 2.014115]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 18/50] [Batch 200/938] [D loss: 0.297141] [G loss: 1.310267]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 18/50] [Batch 400/938] [D loss: 0.223119] [G loss: 1.454390]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 18/50] [Batch 600/938] [D loss: 0.296914] [G loss: 1.644606]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 18/50] [Batch 800/938] [D loss: 0.219574] [G loss: 2.304002]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 19/50] [Batch 0/938] [D loss: 0.260927] [G loss: 2.132348]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 19/50] [Batch 200/938] [D loss: 0.202456] [G loss: 2.009296]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 19/50] [Batch 400/938] [D loss: 0.346010] [G loss: 2.600974]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 19/50] [Batch 600/938] [D loss: 0.366669] [G loss: 1.547614]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 19/50] [Batch 800/938] [D loss: 0.300878] [G loss: 1.608704]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 20/50] [Batch 0/938] [D loss: 0.283505] [G loss: 1.749531]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 20/50] [Batch 200/938] [D loss: 0.336718] [G loss: 1.828097]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 20/50] [Batch 400/938] [D loss: 0.284545] [G loss: 3.541622]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 20/50] [Batch 600/938] [D loss: 0.609171] [G loss: 0.549014]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 20/50] [Batch 800/938] [D loss: 0.250804] [G loss: 1.927366]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 21/50] [Batch 0/938] [D loss: 0.366600] [G loss: 3.371605]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 21/50] [Batch 200/938] [D loss: 0.378647] [G loss: 2.401227]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 21/50] [Batch 400/938] [D loss: 0.450205] [G loss: 2.982776]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 21/50] [Batch 600/938] [D loss: 0.317467] [G loss: 1.638228]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 21/50] [Batch 800/938] [D loss: 0.521899] [G loss: 3.612556]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 22/50] [Batch 0/938] [D loss: 0.332289] [G loss: 1.654595]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 22/50] [Batch 200/938] [D loss: 0.339806] [G loss: 1.091014]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 22/50] [Batch 400/938] [D loss: 0.481209] [G loss: 2.644633]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 22/50] [Batch 600/938] [D loss: 0.435002] [G loss: 2.940613]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 22/50] [Batch 800/938] [D loss: 0.436188] [G loss: 1.415311]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 23/50] [Batch 0/938] [D loss: 0.293921] [G loss: 2.924775]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 23/50] [Batch 200/938] [D loss: 0.242096] [G loss: 2.028823]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 23/50] [Batch 400/938] [D loss: 0.328856] [G loss: 1.280700]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 23/50] [Batch 600/938] [D loss: 0.238783] [G loss: 1.765240]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 23/50] [Batch 800/938] [D loss: 0.317908] [G loss: 1.567961]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 24/50] [Batch 0/938] [D loss: 0.278545] [G loss: 2.046991]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 24/50] [Batch 200/938] [D loss: 0.350835] [G loss: 2.775131]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 24/50] [Batch 400/938] [D loss: 0.500437] [G loss: 0.830708]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 24/50] [Batch 600/938] [D loss: 0.302312] [G loss: 1.997417]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 24/50] [Batch 800/938] [D loss: 0.330815] [G loss: 1.553096]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 25/50] [Batch 0/938] [D loss: 0.324300] [G loss: 1.963142]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 25/50] [Batch 200/938] [D loss: 0.302925] [G loss: 2.129758]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 25/50] [Batch 400/938] [D loss: 0.277354] [G loss: 1.407803]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 25/50] [Batch 600/938] [D loss: 0.446051] [G loss: 2.919940]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 25/50] [Batch 800/938] [D loss: 0.330252] [G loss: 2.785176]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 26/50] [Batch 0/938] [D loss: 0.518839] [G loss: 0.749467]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 26/50] [Batch 200/938] [D loss: 0.328246] [G loss: 2.206290]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 26/50] [Batch 400/938] [D loss: 0.325805] [G loss: 2.467903]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 26/50] [Batch 600/938] [D loss: 0.304172] [G loss: 1.496547]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 26/50] [Batch 800/938] [D loss: 0.333487] [G loss: 1.212913]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 27/50] [Batch 0/938] [D loss: 0.283853] [G loss: 1.456183]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 27/50] [Batch 200/938] [D loss: 0.300700] [G loss: 1.734334]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 27/50] [Batch 400/938] [D loss: 0.223709] [G loss: 1.823399]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 27/50] [Batch 600/938] [D loss: 0.352629] [G loss: 1.403148]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 27/50] [Batch 800/938] [D loss: 0.367255] [G loss: 1.940664]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 28/50] [Batch 0/938] [D loss: 0.305460] [G loss: 1.660203]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 28/50] [Batch 200/938] [D loss: 0.290444] [G loss: 2.630987]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 28/50] [Batch 400/938] [D loss: 0.321738] [G loss: 2.061127]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 28/50] [Batch 600/938] [D loss: 0.340982] [G loss: 1.024038]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 28/50] [Batch 800/938] [D loss: 0.353225] [G loss: 1.128878]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 29/50] [Batch 0/938] [D loss: 0.492247] [G loss: 3.762026]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 29/50] [Batch 200/938] [D loss: 0.356815] [G loss: 1.066372]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 29/50] [Batch 400/938] [D loss: 0.520430] [G loss: 0.747311]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 29/50] [Batch 600/938] [D loss: 0.353294] [G loss: 1.629825]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 29/50] [Batch 800/938] [D loss: 0.255083] [G loss: 1.935950]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 30/50] [Batch 0/938] [D loss: 0.313366] [G loss: 1.989721]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 30/50] [Batch 200/938] [D loss: 0.357662] [G loss: 1.703446]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 30/50] [Batch 400/938] [D loss: 0.319310] [G loss: 2.850029]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 30/50] [Batch 600/938] [D loss: 0.306077] [G loss: 1.911530]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 30/50] [Batch 800/938] [D loss: 0.337216] [G loss: 1.535461]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 31/50] [Batch 0/938] [D loss: 0.479854] [G loss: 3.353958]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 31/50] [Batch 200/938] [D loss: 0.358333] [G loss: 1.728863]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 31/50] [Batch 400/938] [D loss: 0.376688] [G loss: 2.208017]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 31/50] [Batch 600/938] [D loss: 0.281157] [G loss: 2.522438]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 31/50] [Batch 800/938] [D loss: 0.401611] [G loss: 3.114702]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 32/50] [Batch 0/938] [D loss: 0.341068] [G loss: 2.291014]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 32/50] [Batch 200/938] [D loss: 0.259424] [G loss: 1.803163]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 32/50] [Batch 400/938] [D loss: 0.275728] [G loss: 1.412812]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 32/50] [Batch 600/938] [D loss: 0.606178] [G loss: 3.776813]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 32/50] [Batch 800/938] [D loss: 0.334287] [G loss: 1.472541]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 33/50] [Batch 0/938] [D loss: 0.254713] [G loss: 2.293443]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 33/50] [Batch 200/938] [D loss: 0.256726] [G loss: 1.728183]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 33/50] [Batch 400/938] [D loss: 0.386118] [G loss: 3.474566]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 33/50] [Batch 600/938] [D loss: 0.460877] [G loss: 0.998119]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 33/50] [Batch 800/938] [D loss: 0.274899] [G loss: 1.861291]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 34/50] [Batch 0/938] [D loss: 0.297480] [G loss: 2.076593]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 34/50] [Batch 200/938] [D loss: 0.356016] [G loss: 1.621575]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 34/50] [Batch 400/938] [D loss: 0.325527] [G loss: 1.686299]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 34/50] [Batch 600/938] [D loss: 0.255642] [G loss: 2.394156]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 34/50] [Batch 800/938] [D loss: 0.309812] [G loss: 2.405337]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 35/50] [Batch 0/938] [D loss: 0.496929] [G loss: 3.147458]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 35/50] [Batch 200/938] [D loss: 0.272869] [G loss: 1.887674]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 35/50] [Batch 400/938] [D loss: 0.422977] [G loss: 1.018559]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 35/50] [Batch 600/938] [D loss: 0.511871] [G loss: 3.038100]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 35/50] [Batch 800/938] [D loss: 0.606259] [G loss: 0.735702]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 36/50] [Batch 0/938] [D loss: 0.192798] [G loss: 2.251182]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 36/50] [Batch 200/938] [D loss: 0.355101] [G loss: 1.439414]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 36/50] [Batch 400/938] [D loss: 0.243504] [G loss: 2.093222]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 36/50] [Batch 600/938] [D loss: 0.264328] [G loss: 1.838927]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 36/50] [Batch 800/938] [D loss: 0.394532] [G loss: 3.004440]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 37/50] [Batch 0/938] [D loss: 0.440173] [G loss: 2.952553]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 37/50] [Batch 200/938] [D loss: 0.530673] [G loss: 0.744520]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 37/50] [Batch 400/938] [D loss: 0.240035] [G loss: 1.718238]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 37/50] [Batch 600/938] [D loss: 0.385513] [G loss: 1.079470]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 37/50] [Batch 800/938] [D loss: 0.488784] [G loss: 0.844888]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 38/50] [Batch 0/938] [D loss: 0.264483] [G loss: 1.790115]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 38/50] [Batch 200/938] [D loss: 0.341482] [G loss: 1.214312]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 38/50] [Batch 400/938] [D loss: 0.216621] [G loss: 2.092280]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 38/50] [Batch 600/938] [D loss: 0.356749] [G loss: 1.500064]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 38/50] [Batch 800/938] [D loss: 0.213025] [G loss: 2.237987]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 39/50] [Batch 0/938] [D loss: 0.399510] [G loss: 2.065703]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 39/50] [Batch 200/938] [D loss: 0.535493] [G loss: 3.591906]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 39/50] [Batch 400/938] [D loss: 0.345640] [G loss: 1.081534]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 39/50] [Batch 600/938] [D loss: 0.310345] [G loss: 1.929362]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 39/50] [Batch 800/938] [D loss: 0.312244] [G loss: 1.325330]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 40/50] [Batch 0/938] [D loss: 0.391719] [G loss: 2.109133]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 40/50] [Batch 200/938] [D loss: 0.231585] [G loss: 1.782354]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 40/50] [Batch 400/938] [D loss: 0.298365] [G loss: 2.152301]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 40/50] [Batch 600/938] [D loss: 0.229377] [G loss: 1.726703]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 40/50] [Batch 800/938] [D loss: 0.357854] [G loss: 1.203347]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 41/50] [Batch 0/938] [D loss: 0.264049] [G loss: 1.606629]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 41/50] [Batch 200/938] [D loss: 0.370407] [G loss: 1.501956]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 41/50] [Batch 400/938] [D loss: 0.316334] [G loss: 1.224799]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 41/50] [Batch 600/938] [D loss: 0.273654] [G loss: 2.262811]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 41/50] [Batch 800/938] [D loss: 0.286970] [G loss: 2.670088]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 42/50] [Batch 0/938] [D loss: 0.450730] [G loss: 0.897023]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 42/50] [Batch 200/938] [D loss: 0.277622] [G loss: 1.736794]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 42/50] [Batch 400/938] [D loss: 0.264000] [G loss: 1.354831]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 42/50] [Batch 600/938] [D loss: 0.308428] [G loss: 2.523836]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 42/50] [Batch 800/938] [D loss: 0.238168] [G loss: 2.509701]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 43/50] [Batch 0/938] [D loss: 0.274761] [G loss: 1.936363]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 43/50] [Batch 200/938] [D loss: 0.250282] [G loss: 1.627299]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 43/50] [Batch 400/938] [D loss: 0.287915] [G loss: 1.461215]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 43/50] [Batch 600/938] [D loss: 0.283187] [G loss: 1.673577]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 43/50] [Batch 800/938] [D loss: 0.258294] [G loss: 1.858933]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 44/50] [Batch 0/938] [D loss: 0.346939] [G loss: 1.189131]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 44/50] [Batch 200/938] [D loss: 0.339501] [G loss: 1.652311]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 44/50] [Batch 400/938] [D loss: 0.398718] [G loss: 3.665866]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 44/50] [Batch 600/938] [D loss: 0.282336] [G loss: 2.359506]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 44/50] [Batch 800/938] [D loss: 0.316720] [G loss: 2.319956]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 45/50] [Batch 0/938] [D loss: 0.220595] [G loss: 2.395647]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 45/50] [Batch 200/938] [D loss: 0.489747] [G loss: 3.274275]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 45/50] [Batch 400/938] [D loss: 0.383706] [G loss: 3.144788]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 45/50] [Batch 600/938] [D loss: 0.399844] [G loss: 1.772309]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 45/50] [Batch 800/938] [D loss: 0.386925] [G loss: 1.742795]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 46/50] [Batch 0/938] [D loss: 0.324661] [G loss: 2.123533]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 46/50] [Batch 200/938] [D loss: 0.264715] [G loss: 1.823303]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 46/50] [Batch 400/938] [D loss: 0.297714] [G loss: 2.160267]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 46/50] [Batch 600/938] [D loss: 0.303799] [G loss: 2.046722]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 46/50] [Batch 800/938] [D loss: 0.299082] [G loss: 1.438635]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 47/50] [Batch 0/938] [D loss: 0.275386] [G loss: 2.609584]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 47/50] [Batch 200/938] [D loss: 0.349731] [G loss: 1.838132]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 47/50] [Batch 400/938] [D loss: 0.311837] [G loss: 2.032070]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 47/50] [Batch 600/938] [D loss: 0.376685] [G loss: 2.930089]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 47/50] [Batch 800/938] [D loss: 0.317640] [G loss: 1.348710]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 48/50] [Batch 0/938] [D loss: 0.278143] [G loss: 1.705037]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 48/50] [Batch 200/938] [D loss: 0.197991] [G loss: 2.372166]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 48/50] [Batch 400/938] [D loss: 0.256488] [G loss: 1.424960]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 48/50] [Batch 600/938] [D loss: 0.280789] [G loss: 1.880407]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 48/50] [Batch 800/938] [D loss: 0.278460] [G loss: 2.001757]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 49/50] [Batch 0/938] [D loss: 0.283746] [G loss: 1.620291]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 49/50] [Batch 200/938] [D loss: 0.345367] [G loss: 1.358737]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 49/50] [Batch 400/938] [D loss: 0.222069] [G loss: 2.920829]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 49/50] [Batch 600/938] [D loss: 0.377115] [G loss: 1.568750]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 49/50] [Batch 800/938] [D loss: 0.299248] [G loss: 2.678013]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 50/50] [Batch 0/938] [D loss: 0.295601] [G loss: 2.524555]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 50/50] [Batch 200/938] [D loss: 0.322502] [G loss: 2.409538]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 50/50] [Batch 400/938] [D loss: 0.238117] [G loss: 2.168916]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 50/50] [Batch 600/938] [D loss: 0.296904] [G loss: 1.372419]\n",
            "torch.Size([64, 1, 28, 28])\n",
            "[Epoch 50/50] [Batch 800/938] [D loss: 0.314837] [G loss: 2.062786]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1LSaGYwXi8G"
      },
      "source": [
        "The next step is to implement spectral normalisation. You can also work on the code for the Wasserstein loss first, if you find that more appealing. But be aware that in order to keep the Lipschitz constraint you need to clamp the weights instead. Even the Wasserstein paper itself mentions that this is a terrible way of enforcing the constraint. So we suggest thinking about how spectral normalisation works first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhc4Yz90H9ok",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "45120b43-d32c-4fc0-be80-7e0889f63c97"
      },
      "source": [
        "\n",
        "# The spectral norm is from this source: \n",
        "# https://github.com/christiancosgrove/pytorch-spectral-normalization-gan\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "\n",
        "def normalize(v, eps=1e-12):\n",
        "    # scales the vector v to unit length \n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    ''' \n",
        "    You can use SpectralNorm() just like other normalisation layer.\n",
        "    Example:\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        SpectralNorm(layer1),\n",
        "        nn.ReLU(),\n",
        "        SpectralNorm(layer2),\n",
        "        nn.ReLU(),\n",
        "        SpectralNorm(layer3),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "    '''\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        self.sigma = 0 #erase this\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        # work on this method and fill in the gaps\n",
        "      \n",
        "        # getattr is an easy way to get a parameter of the module by giving its name\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        # the matrix W\n",
        "        w_matrix = w.view(height,-1)\n",
        "        \n",
        "        for _ in range(self.power_iterations):\n",
        "            # .data refers to the underlying tensor of the parameter\n",
        "            # this is only important when assigning a new tensor to a parameter\n",
        "            # in this way you don't overwrite the parameter with a tensor.\n",
        "            # When calling a parameter for computation you can also use .data \n",
        "            # but it is not necessary\n",
        "            \n",
        "            # implement one step of power iteration\n",
        "            # you can use @ for matrix multiplication (notice vectors are also matrices)\n",
        "            # and torch.t(w) to get the transpose matrix of w\n",
        "            \n",
        "            v.data = normalize(torch.t(w_matrix)@u) #fill in this gap\n",
        "            u.data = normalize(w_matrix@v) #fill in this gap\n",
        "        \n",
        "        # This is sigma(W) as defined in the blog post. What does it represent? \n",
        "        sigma = u@w_matrix@v #fill in this gap\n",
        "        self.sigma = sigma #erase this\n",
        "        \n",
        "        # w_norm should be the (spectral) normalised version of \n",
        "        # the weight of the module\n",
        "        \n",
        "        w_norm = w / sigma.expand_as(w) #fill in this gap\n",
        "        setattr(self.module, self.name, w_norm)\n",
        "\n",
        "    def _made_params(self):\n",
        "        # checks if the parameters already exist\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        # initialize the parameters \n",
        "        w = getattr(self.module, self.name) #returns the weight of the module\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "        \n",
        "        # set the parameters u,v randomly (with a standard normal distribution)\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        \n",
        "        # scale them to unit length\n",
        "        u.data = normalize(u.data)\n",
        "        v.data = normalize(v.data)\n",
        "        \n",
        "        # a copy of the weight parameter\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nYou can use SpectralNorm() just like other normalisation layer.\\nExample:\\n\\nmodel = nn.Sequential(\\n    SpectralNorm(layer1),\\n    nn.ReLU(),\\n    SpectralNorm(layer2),\\n    nn.ReLU(),\\n    SpectralNorm(layer3),\\n    nn.Sigmoid(),\\n)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO5JUoJ0kspZ"
      },
      "source": [
        "Now we are going to implement both spectral normalisation and the Wasserstein loss into the GAN. You will recognize that the code is the same as above with a few minor exceptions. Think about where the spectral normalisation should be used. Then take a look at the WGAN paper. They explicitly say how both the loss of the generator and discriminator should be computed. When you first understand it you will notice how easy this task actually is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ziu6VTBvRdxO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d44c2d0-fac9-4a9e-a8d2-f1cfda03c5bc"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "b1 = 0.5              #adam: decay of first order momentum of gradient\n",
        "b2 = 0.999            #adam: decay of second order momentum of gradient\n",
        "n_epochs = 50         #number of epochs of training\n",
        "batch_size = 64       #size of the batches\n",
        "lr = 0.0001           #adam: learning rate\n",
        "n_cpu = 8,            #number of cpu threads to use during batch generation\n",
        "latent_dim = 100      #dimensionality of the latent space\n",
        "img_size = 28         #size of each image dimension\n",
        "channels = 1          #number of image channels\n",
        "sample_interval = 400 #interval between image samples\n",
        "n_critic = 1\n",
        "\n",
        "\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "random_seed = 1\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "class W_Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(W_Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class W_Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(W_Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            SpectralNorm(nn.Linear(int(np.prod(img_shape)), 512)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SpectralNorm(nn.Linear(512, 256)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SpectralNorm(nn.Linear(256, 1)),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "w_generator = W_Generator()\n",
        "w_discriminator = W_Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    w_generator.cuda()\n",
        "    w_discriminator.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"../../data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "w_optimizer_G = torch.optim.Adam(w_generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "w_optimizer_D = torch.optim.Adam(w_discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "\n",
        "       \n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = w_generator(z) #fake_imgs = generator(z).detach()\n",
        "        \n",
        "        \n",
        "        # -------------------------------------\n",
        "        #  Train Generator every n_critic steps\n",
        "        # -------------------------------------\n",
        "        \n",
        "        \n",
        "        if i%n_critic == 0:\n",
        "          \n",
        "            #gen_imgs = generator(z)\n",
        "            \n",
        "            w_optimizer_G.zero_grad()\n",
        "            \n",
        "            # Loss measures generator's ability to fool the discriminator\n",
        "            g_loss = -torch.mean(w_discriminator(gen_imgs))\n",
        "\n",
        "            g_loss.backward()\n",
        "            w_optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        w_optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        # Use .detach() for the generated images to not compute another gradient graph \n",
        "        d_loss = - torch.mean(w_discriminator(real_imgs)) + torch.mean(w_discriminator(gen_imgs.detach()))\n",
        "\n",
        "        d_loss.backward()\n",
        "        w_optimizer_D.step()\n",
        "        if i%200 == 0:\n",
        "          print(\n",
        "              \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "              % (epoch+1, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        )\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % sample_interval == 0:\n",
        "            save_image(gen_imgs.data[:25], \"images/WGAN-%d.png\" % batches_done, nrow=5, normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1/50] [Batch 0/938] [D loss: -0.014487] [G loss: -0.499407]\n",
            "[Epoch 1/50] [Batch 200/938] [D loss: -0.318131] [G loss: -0.415803]\n",
            "[Epoch 1/50] [Batch 400/938] [D loss: -0.417677] [G loss: -0.387452]\n",
            "[Epoch 1/50] [Batch 600/938] [D loss: -0.452891] [G loss: -0.340096]\n",
            "[Epoch 1/50] [Batch 800/938] [D loss: -0.485480] [G loss: -0.296260]\n",
            "[Epoch 2/50] [Batch 0/938] [D loss: -0.478660] [G loss: -0.259381]\n",
            "[Epoch 2/50] [Batch 200/938] [D loss: -0.506564] [G loss: -0.250538]\n",
            "[Epoch 2/50] [Batch 400/938] [D loss: -0.476503] [G loss: -0.244763]\n",
            "[Epoch 2/50] [Batch 600/938] [D loss: -0.506104] [G loss: -0.255101]\n",
            "[Epoch 2/50] [Batch 800/938] [D loss: -0.539549] [G loss: -0.255530]\n",
            "[Epoch 3/50] [Batch 0/938] [D loss: -0.522240] [G loss: -0.258959]\n",
            "[Epoch 3/50] [Batch 200/938] [D loss: -0.495374] [G loss: -0.251975]\n",
            "[Epoch 3/50] [Batch 400/938] [D loss: -0.515602] [G loss: -0.269423]\n",
            "[Epoch 3/50] [Batch 600/938] [D loss: -0.510894] [G loss: -0.201481]\n",
            "[Epoch 3/50] [Batch 800/938] [D loss: -0.524746] [G loss: -0.257854]\n",
            "[Epoch 4/50] [Batch 0/938] [D loss: -0.470833] [G loss: -0.286027]\n",
            "[Epoch 4/50] [Batch 200/938] [D loss: -0.485491] [G loss: -0.245078]\n",
            "[Epoch 4/50] [Batch 400/938] [D loss: -0.453939] [G loss: -0.266797]\n",
            "[Epoch 4/50] [Batch 600/938] [D loss: -0.420212] [G loss: -0.365872]\n",
            "[Epoch 4/50] [Batch 800/938] [D loss: -0.439358] [G loss: -0.284136]\n",
            "[Epoch 5/50] [Batch 0/938] [D loss: -0.436694] [G loss: -0.277299]\n",
            "[Epoch 5/50] [Batch 200/938] [D loss: -0.410463] [G loss: -0.280609]\n",
            "[Epoch 5/50] [Batch 400/938] [D loss: -0.407285] [G loss: -0.287270]\n",
            "[Epoch 5/50] [Batch 600/938] [D loss: -0.401718] [G loss: -0.241191]\n",
            "[Epoch 5/50] [Batch 800/938] [D loss: -0.393887] [G loss: -0.326031]\n",
            "[Epoch 6/50] [Batch 0/938] [D loss: -0.347229] [G loss: -0.338190]\n",
            "[Epoch 6/50] [Batch 200/938] [D loss: -0.358017] [G loss: -0.332739]\n",
            "[Epoch 6/50] [Batch 400/938] [D loss: -0.390515] [G loss: -0.321073]\n",
            "[Epoch 6/50] [Batch 600/938] [D loss: -0.373787] [G loss: -0.313137]\n",
            "[Epoch 6/50] [Batch 800/938] [D loss: -0.347007] [G loss: -0.343838]\n",
            "[Epoch 7/50] [Batch 0/938] [D loss: -0.328049] [G loss: -0.342267]\n",
            "[Epoch 7/50] [Batch 200/938] [D loss: -0.328844] [G loss: -0.356689]\n",
            "[Epoch 7/50] [Batch 400/938] [D loss: -0.310233] [G loss: -0.334176]\n",
            "[Epoch 7/50] [Batch 600/938] [D loss: -0.328653] [G loss: -0.327134]\n",
            "[Epoch 7/50] [Batch 800/938] [D loss: -0.302096] [G loss: -0.341845]\n",
            "[Epoch 8/50] [Batch 0/938] [D loss: -0.327159] [G loss: -0.306199]\n",
            "[Epoch 8/50] [Batch 200/938] [D loss: -0.287209] [G loss: -0.367997]\n",
            "[Epoch 8/50] [Batch 400/938] [D loss: -0.298842] [G loss: -0.350935]\n",
            "[Epoch 8/50] [Batch 600/938] [D loss: -0.269836] [G loss: -0.362498]\n",
            "[Epoch 8/50] [Batch 800/938] [D loss: -0.247485] [G loss: -0.379590]\n",
            "[Epoch 9/50] [Batch 0/938] [D loss: -0.237563] [G loss: -0.344160]\n",
            "[Epoch 9/50] [Batch 200/938] [D loss: -0.285663] [G loss: -0.343280]\n",
            "[Epoch 9/50] [Batch 400/938] [D loss: -0.275415] [G loss: -0.364903]\n",
            "[Epoch 9/50] [Batch 600/938] [D loss: -0.295214] [G loss: -0.351640]\n",
            "[Epoch 9/50] [Batch 800/938] [D loss: -0.254773] [G loss: -0.369614]\n",
            "[Epoch 10/50] [Batch 0/938] [D loss: -0.215069] [G loss: -0.364696]\n",
            "[Epoch 10/50] [Batch 200/938] [D loss: -0.236284] [G loss: -0.389099]\n",
            "[Epoch 10/50] [Batch 400/938] [D loss: -0.210536] [G loss: -0.391316]\n",
            "[Epoch 10/50] [Batch 600/938] [D loss: -0.247449] [G loss: -0.360480]\n",
            "[Epoch 10/50] [Batch 800/938] [D loss: -0.222248] [G loss: -0.382087]\n",
            "[Epoch 11/50] [Batch 0/938] [D loss: -0.232564] [G loss: -0.382394]\n",
            "[Epoch 11/50] [Batch 200/938] [D loss: -0.221569] [G loss: -0.372964]\n",
            "[Epoch 11/50] [Batch 400/938] [D loss: -0.212893] [G loss: -0.412759]\n",
            "[Epoch 11/50] [Batch 600/938] [D loss: -0.216429] [G loss: -0.389266]\n",
            "[Epoch 11/50] [Batch 800/938] [D loss: -0.229629] [G loss: -0.373869]\n",
            "[Epoch 12/50] [Batch 0/938] [D loss: -0.175863] [G loss: -0.378400]\n",
            "[Epoch 12/50] [Batch 200/938] [D loss: -0.180830] [G loss: -0.398968]\n",
            "[Epoch 12/50] [Batch 400/938] [D loss: -0.201201] [G loss: -0.374168]\n",
            "[Epoch 12/50] [Batch 600/938] [D loss: -0.193219] [G loss: -0.389231]\n",
            "[Epoch 12/50] [Batch 800/938] [D loss: -0.177005] [G loss: -0.416247]\n",
            "[Epoch 13/50] [Batch 0/938] [D loss: -0.142806] [G loss: -0.410862]\n",
            "[Epoch 13/50] [Batch 200/938] [D loss: -0.193110] [G loss: -0.399675]\n",
            "[Epoch 13/50] [Batch 400/938] [D loss: -0.165967] [G loss: -0.415292]\n",
            "[Epoch 13/50] [Batch 600/938] [D loss: -0.161501] [G loss: -0.400430]\n",
            "[Epoch 13/50] [Batch 800/938] [D loss: -0.166437] [G loss: -0.421210]\n",
            "[Epoch 14/50] [Batch 0/938] [D loss: -0.178466] [G loss: -0.406324]\n",
            "[Epoch 14/50] [Batch 200/938] [D loss: -0.170482] [G loss: -0.391021]\n",
            "[Epoch 14/50] [Batch 400/938] [D loss: -0.166294] [G loss: -0.426206]\n",
            "[Epoch 14/50] [Batch 600/938] [D loss: -0.157245] [G loss: -0.426244]\n",
            "[Epoch 14/50] [Batch 800/938] [D loss: -0.133652] [G loss: -0.419426]\n",
            "[Epoch 15/50] [Batch 0/938] [D loss: -0.181827] [G loss: -0.427599]\n",
            "[Epoch 15/50] [Batch 200/938] [D loss: -0.144069] [G loss: -0.407974]\n",
            "[Epoch 15/50] [Batch 400/938] [D loss: -0.151334] [G loss: -0.446566]\n",
            "[Epoch 15/50] [Batch 600/938] [D loss: -0.145156] [G loss: -0.423226]\n",
            "[Epoch 15/50] [Batch 800/938] [D loss: -0.156212] [G loss: -0.432294]\n",
            "[Epoch 16/50] [Batch 0/938] [D loss: -0.152131] [G loss: -0.439333]\n",
            "[Epoch 16/50] [Batch 200/938] [D loss: -0.127179] [G loss: -0.439669]\n",
            "[Epoch 16/50] [Batch 400/938] [D loss: -0.145376] [G loss: -0.399254]\n",
            "[Epoch 16/50] [Batch 600/938] [D loss: -0.129369] [G loss: -0.433263]\n",
            "[Epoch 16/50] [Batch 800/938] [D loss: -0.117123] [G loss: -0.451862]\n",
            "[Epoch 17/50] [Batch 0/938] [D loss: -0.120401] [G loss: -0.428346]\n",
            "[Epoch 17/50] [Batch 200/938] [D loss: -0.129641] [G loss: -0.423778]\n",
            "[Epoch 17/50] [Batch 400/938] [D loss: -0.131849] [G loss: -0.408917]\n",
            "[Epoch 17/50] [Batch 600/938] [D loss: -0.147197] [G loss: -0.433683]\n",
            "[Epoch 17/50] [Batch 800/938] [D loss: -0.104674] [G loss: -0.450173]\n",
            "[Epoch 18/50] [Batch 0/938] [D loss: -0.124167] [G loss: -0.444617]\n",
            "[Epoch 18/50] [Batch 200/938] [D loss: -0.102361] [G loss: -0.457984]\n",
            "[Epoch 18/50] [Batch 400/938] [D loss: -0.116837] [G loss: -0.448729]\n",
            "[Epoch 18/50] [Batch 600/938] [D loss: -0.094380] [G loss: -0.414033]\n",
            "[Epoch 18/50] [Batch 800/938] [D loss: -0.122008] [G loss: -0.443479]\n",
            "[Epoch 19/50] [Batch 0/938] [D loss: -0.114783] [G loss: -0.445802]\n",
            "[Epoch 19/50] [Batch 200/938] [D loss: -0.129866] [G loss: -0.418705]\n",
            "[Epoch 19/50] [Batch 400/938] [D loss: -0.106401] [G loss: -0.459390]\n",
            "[Epoch 19/50] [Batch 600/938] [D loss: -0.103105] [G loss: -0.393697]\n",
            "[Epoch 19/50] [Batch 800/938] [D loss: -0.093878] [G loss: -0.454870]\n",
            "[Epoch 20/50] [Batch 0/938] [D loss: -0.105632] [G loss: -0.464734]\n",
            "[Epoch 20/50] [Batch 200/938] [D loss: -0.118191] [G loss: -0.435345]\n",
            "[Epoch 20/50] [Batch 400/938] [D loss: -0.112660] [G loss: -0.434271]\n",
            "[Epoch 20/50] [Batch 600/938] [D loss: -0.112261] [G loss: -0.437543]\n",
            "[Epoch 20/50] [Batch 800/938] [D loss: -0.113335] [G loss: -0.471553]\n",
            "[Epoch 21/50] [Batch 0/938] [D loss: -0.103717] [G loss: -0.452385]\n",
            "[Epoch 21/50] [Batch 200/938] [D loss: -0.119927] [G loss: -0.412992]\n",
            "[Epoch 21/50] [Batch 400/938] [D loss: -0.107547] [G loss: -0.437249]\n",
            "[Epoch 21/50] [Batch 600/938] [D loss: -0.129179] [G loss: -0.432862]\n",
            "[Epoch 21/50] [Batch 800/938] [D loss: -0.107465] [G loss: -0.440381]\n",
            "[Epoch 22/50] [Batch 0/938] [D loss: -0.102781] [G loss: -0.451020]\n",
            "[Epoch 22/50] [Batch 200/938] [D loss: -0.102929] [G loss: -0.428886]\n",
            "[Epoch 22/50] [Batch 400/938] [D loss: -0.076802] [G loss: -0.462712]\n",
            "[Epoch 22/50] [Batch 600/938] [D loss: -0.095519] [G loss: -0.432968]\n",
            "[Epoch 22/50] [Batch 800/938] [D loss: -0.087930] [G loss: -0.439668]\n",
            "[Epoch 23/50] [Batch 0/938] [D loss: -0.096656] [G loss: -0.440349]\n",
            "[Epoch 23/50] [Batch 200/938] [D loss: -0.097061] [G loss: -0.414788]\n",
            "[Epoch 23/50] [Batch 400/938] [D loss: -0.127918] [G loss: -0.421668]\n",
            "[Epoch 23/50] [Batch 600/938] [D loss: -0.073026] [G loss: -0.453070]\n",
            "[Epoch 23/50] [Batch 800/938] [D loss: -0.107980] [G loss: -0.436176]\n",
            "[Epoch 24/50] [Batch 0/938] [D loss: -0.111727] [G loss: -0.495281]\n",
            "[Epoch 24/50] [Batch 200/938] [D loss: -0.109051] [G loss: -0.421554]\n",
            "[Epoch 24/50] [Batch 400/938] [D loss: -0.101558] [G loss: -0.452199]\n",
            "[Epoch 24/50] [Batch 600/938] [D loss: -0.089346] [G loss: -0.457873]\n",
            "[Epoch 24/50] [Batch 800/938] [D loss: -0.091902] [G loss: -0.458307]\n",
            "[Epoch 25/50] [Batch 0/938] [D loss: -0.084606] [G loss: -0.457872]\n",
            "[Epoch 25/50] [Batch 200/938] [D loss: -0.098688] [G loss: -0.446587]\n",
            "[Epoch 25/50] [Batch 400/938] [D loss: -0.086888] [G loss: -0.434939]\n",
            "[Epoch 25/50] [Batch 600/938] [D loss: -0.103088] [G loss: -0.423609]\n",
            "[Epoch 25/50] [Batch 800/938] [D loss: -0.083552] [G loss: -0.467447]\n",
            "[Epoch 26/50] [Batch 0/938] [D loss: -0.071521] [G loss: -0.469451]\n",
            "[Epoch 26/50] [Batch 200/938] [D loss: -0.084445] [G loss: -0.440295]\n",
            "[Epoch 26/50] [Batch 400/938] [D loss: -0.074142] [G loss: -0.425713]\n",
            "[Epoch 26/50] [Batch 600/938] [D loss: -0.061742] [G loss: -0.451412]\n",
            "[Epoch 26/50] [Batch 800/938] [D loss: -0.085222] [G loss: -0.467503]\n",
            "[Epoch 27/50] [Batch 0/938] [D loss: -0.070871] [G loss: -0.464642]\n",
            "[Epoch 27/50] [Batch 200/938] [D loss: -0.074831] [G loss: -0.465940]\n",
            "[Epoch 27/50] [Batch 400/938] [D loss: -0.111430] [G loss: -0.441997]\n",
            "[Epoch 27/50] [Batch 600/938] [D loss: -0.093811] [G loss: -0.431777]\n",
            "[Epoch 27/50] [Batch 800/938] [D loss: -0.041951] [G loss: -0.475373]\n",
            "[Epoch 28/50] [Batch 0/938] [D loss: -0.072747] [G loss: -0.442718]\n",
            "[Epoch 28/50] [Batch 200/938] [D loss: -0.083404] [G loss: -0.424193]\n",
            "[Epoch 28/50] [Batch 400/938] [D loss: -0.065184] [G loss: -0.402250]\n",
            "[Epoch 28/50] [Batch 600/938] [D loss: -0.084623] [G loss: -0.457421]\n",
            "[Epoch 28/50] [Batch 800/938] [D loss: -0.101641] [G loss: -0.443088]\n",
            "[Epoch 29/50] [Batch 0/938] [D loss: -0.076963] [G loss: -0.481550]\n",
            "[Epoch 29/50] [Batch 200/938] [D loss: -0.083562] [G loss: -0.438477]\n",
            "[Epoch 29/50] [Batch 400/938] [D loss: -0.090718] [G loss: -0.419190]\n",
            "[Epoch 29/50] [Batch 600/938] [D loss: -0.055102] [G loss: -0.497156]\n",
            "[Epoch 29/50] [Batch 800/938] [D loss: -0.066627] [G loss: -0.453561]\n",
            "[Epoch 30/50] [Batch 0/938] [D loss: -0.095691] [G loss: -0.431496]\n",
            "[Epoch 30/50] [Batch 200/938] [D loss: -0.103075] [G loss: -0.501685]\n",
            "[Epoch 30/50] [Batch 400/938] [D loss: -0.093675] [G loss: -0.425590]\n",
            "[Epoch 30/50] [Batch 600/938] [D loss: -0.070448] [G loss: -0.435689]\n",
            "[Epoch 30/50] [Batch 800/938] [D loss: -0.108954] [G loss: -0.463714]\n",
            "[Epoch 31/50] [Batch 0/938] [D loss: -0.049743] [G loss: -0.480507]\n",
            "[Epoch 31/50] [Batch 200/938] [D loss: -0.090813] [G loss: -0.460042]\n",
            "[Epoch 31/50] [Batch 400/938] [D loss: -0.072817] [G loss: -0.458958]\n",
            "[Epoch 31/50] [Batch 600/938] [D loss: -0.031624] [G loss: -0.433104]\n",
            "[Epoch 31/50] [Batch 800/938] [D loss: -0.091066] [G loss: -0.473282]\n",
            "[Epoch 32/50] [Batch 0/938] [D loss: -0.080572] [G loss: -0.470156]\n",
            "[Epoch 32/50] [Batch 200/938] [D loss: -0.065511] [G loss: -0.454659]\n",
            "[Epoch 32/50] [Batch 400/938] [D loss: -0.095508] [G loss: -0.463424]\n",
            "[Epoch 32/50] [Batch 600/938] [D loss: -0.074613] [G loss: -0.469007]\n",
            "[Epoch 32/50] [Batch 800/938] [D loss: -0.100242] [G loss: -0.446094]\n",
            "[Epoch 33/50] [Batch 0/938] [D loss: -0.092609] [G loss: -0.470903]\n",
            "[Epoch 33/50] [Batch 200/938] [D loss: -0.070677] [G loss: -0.464283]\n",
            "[Epoch 33/50] [Batch 400/938] [D loss: -0.067494] [G loss: -0.430236]\n",
            "[Epoch 33/50] [Batch 600/938] [D loss: -0.074351] [G loss: -0.432377]\n",
            "[Epoch 33/50] [Batch 800/938] [D loss: -0.072895] [G loss: -0.439579]\n",
            "[Epoch 34/50] [Batch 0/938] [D loss: -0.064346] [G loss: -0.459411]\n",
            "[Epoch 34/50] [Batch 200/938] [D loss: -0.058968] [G loss: -0.416136]\n",
            "[Epoch 34/50] [Batch 400/938] [D loss: -0.087067] [G loss: -0.431039]\n",
            "[Epoch 34/50] [Batch 600/938] [D loss: -0.073490] [G loss: -0.456861]\n",
            "[Epoch 34/50] [Batch 800/938] [D loss: -0.081147] [G loss: -0.464977]\n",
            "[Epoch 35/50] [Batch 0/938] [D loss: -0.090630] [G loss: -0.448547]\n",
            "[Epoch 35/50] [Batch 200/938] [D loss: -0.067831] [G loss: -0.475009]\n",
            "[Epoch 35/50] [Batch 400/938] [D loss: -0.068823] [G loss: -0.488063]\n",
            "[Epoch 35/50] [Batch 600/938] [D loss: -0.063329] [G loss: -0.397664]\n",
            "[Epoch 35/50] [Batch 800/938] [D loss: -0.047356] [G loss: -0.461374]\n",
            "[Epoch 36/50] [Batch 0/938] [D loss: -0.068102] [G loss: -0.405428]\n",
            "[Epoch 36/50] [Batch 200/938] [D loss: -0.045207] [G loss: -0.458988]\n",
            "[Epoch 36/50] [Batch 400/938] [D loss: -0.064996] [G loss: -0.458141]\n",
            "[Epoch 36/50] [Batch 600/938] [D loss: -0.083726] [G loss: -0.455726]\n",
            "[Epoch 36/50] [Batch 800/938] [D loss: -0.076546] [G loss: -0.417897]\n",
            "[Epoch 37/50] [Batch 0/938] [D loss: -0.075187] [G loss: -0.465913]\n",
            "[Epoch 37/50] [Batch 200/938] [D loss: -0.074842] [G loss: -0.463899]\n",
            "[Epoch 37/50] [Batch 400/938] [D loss: -0.056642] [G loss: -0.515798]\n",
            "[Epoch 37/50] [Batch 600/938] [D loss: -0.084916] [G loss: -0.479023]\n",
            "[Epoch 37/50] [Batch 800/938] [D loss: -0.084896] [G loss: -0.437833]\n",
            "[Epoch 38/50] [Batch 0/938] [D loss: -0.060759] [G loss: -0.431581]\n",
            "[Epoch 38/50] [Batch 200/938] [D loss: -0.069588] [G loss: -0.463861]\n",
            "[Epoch 38/50] [Batch 400/938] [D loss: -0.092009] [G loss: -0.470620]\n",
            "[Epoch 38/50] [Batch 600/938] [D loss: -0.082057] [G loss: -0.463375]\n",
            "[Epoch 38/50] [Batch 800/938] [D loss: -0.058904] [G loss: -0.501162]\n",
            "[Epoch 39/50] [Batch 0/938] [D loss: -0.052880] [G loss: -0.494490]\n",
            "[Epoch 39/50] [Batch 200/938] [D loss: -0.060332] [G loss: -0.441290]\n",
            "[Epoch 39/50] [Batch 400/938] [D loss: -0.084358] [G loss: -0.484470]\n",
            "[Epoch 39/50] [Batch 600/938] [D loss: -0.072129] [G loss: -0.442650]\n",
            "[Epoch 39/50] [Batch 800/938] [D loss: -0.066405] [G loss: -0.474181]\n",
            "[Epoch 40/50] [Batch 0/938] [D loss: -0.080925] [G loss: -0.434546]\n",
            "[Epoch 40/50] [Batch 200/938] [D loss: -0.093968] [G loss: -0.441128]\n",
            "[Epoch 40/50] [Batch 400/938] [D loss: -0.074401] [G loss: -0.514453]\n",
            "[Epoch 40/50] [Batch 600/938] [D loss: -0.079610] [G loss: -0.443095]\n",
            "[Epoch 40/50] [Batch 800/938] [D loss: -0.058522] [G loss: -0.463903]\n",
            "[Epoch 41/50] [Batch 0/938] [D loss: -0.071602] [G loss: -0.447053]\n",
            "[Epoch 41/50] [Batch 200/938] [D loss: -0.062793] [G loss: -0.414569]\n",
            "[Epoch 41/50] [Batch 400/938] [D loss: -0.081597] [G loss: -0.465573]\n",
            "[Epoch 41/50] [Batch 600/938] [D loss: -0.053273] [G loss: -0.498245]\n",
            "[Epoch 41/50] [Batch 800/938] [D loss: -0.078252] [G loss: -0.501150]\n",
            "[Epoch 42/50] [Batch 0/938] [D loss: -0.086142] [G loss: -0.464975]\n",
            "[Epoch 42/50] [Batch 200/938] [D loss: -0.092748] [G loss: -0.459728]\n",
            "[Epoch 42/50] [Batch 400/938] [D loss: -0.074368] [G loss: -0.472368]\n",
            "[Epoch 42/50] [Batch 600/938] [D loss: -0.091493] [G loss: -0.464063]\n",
            "[Epoch 42/50] [Batch 800/938] [D loss: -0.080353] [G loss: -0.451741]\n",
            "[Epoch 43/50] [Batch 0/938] [D loss: -0.062583] [G loss: -0.481727]\n",
            "[Epoch 43/50] [Batch 200/938] [D loss: -0.049949] [G loss: -0.487815]\n",
            "[Epoch 43/50] [Batch 400/938] [D loss: -0.034235] [G loss: -0.485056]\n",
            "[Epoch 43/50] [Batch 600/938] [D loss: -0.068377] [G loss: -0.469274]\n",
            "[Epoch 43/50] [Batch 800/938] [D loss: -0.076633] [G loss: -0.543572]\n",
            "[Epoch 44/50] [Batch 0/938] [D loss: -0.066102] [G loss: -0.470967]\n",
            "[Epoch 44/50] [Batch 200/938] [D loss: -0.074108] [G loss: -0.467375]\n",
            "[Epoch 44/50] [Batch 400/938] [D loss: -0.083428] [G loss: -0.466890]\n",
            "[Epoch 44/50] [Batch 600/938] [D loss: -0.060859] [G loss: -0.471394]\n",
            "[Epoch 44/50] [Batch 800/938] [D loss: -0.079018] [G loss: -0.446503]\n",
            "[Epoch 45/50] [Batch 0/938] [D loss: -0.066292] [G loss: -0.489702]\n",
            "[Epoch 45/50] [Batch 200/938] [D loss: -0.064598] [G loss: -0.417863]\n",
            "[Epoch 45/50] [Batch 400/938] [D loss: -0.035849] [G loss: -0.499089]\n",
            "[Epoch 45/50] [Batch 600/938] [D loss: -0.045277] [G loss: -0.461278]\n",
            "[Epoch 45/50] [Batch 800/938] [D loss: -0.078212] [G loss: -0.473351]\n",
            "[Epoch 46/50] [Batch 0/938] [D loss: -0.048407] [G loss: -0.419193]\n",
            "[Epoch 46/50] [Batch 200/938] [D loss: -0.037656] [G loss: -0.458957]\n",
            "[Epoch 46/50] [Batch 400/938] [D loss: -0.051328] [G loss: -0.509964]\n",
            "[Epoch 46/50] [Batch 600/938] [D loss: -0.061734] [G loss: -0.432114]\n",
            "[Epoch 46/50] [Batch 800/938] [D loss: -0.029132] [G loss: -0.452362]\n",
            "[Epoch 47/50] [Batch 0/938] [D loss: -0.045468] [G loss: -0.475953]\n",
            "[Epoch 47/50] [Batch 200/938] [D loss: -0.072201] [G loss: -0.496598]\n",
            "[Epoch 47/50] [Batch 400/938] [D loss: -0.076163] [G loss: -0.471438]\n",
            "[Epoch 47/50] [Batch 600/938] [D loss: -0.067072] [G loss: -0.462427]\n",
            "[Epoch 47/50] [Batch 800/938] [D loss: -0.060162] [G loss: -0.449286]\n",
            "[Epoch 48/50] [Batch 0/938] [D loss: -0.057548] [G loss: -0.457830]\n",
            "[Epoch 48/50] [Batch 200/938] [D loss: -0.083168] [G loss: -0.433097]\n",
            "[Epoch 48/50] [Batch 400/938] [D loss: -0.083074] [G loss: -0.445267]\n",
            "[Epoch 48/50] [Batch 600/938] [D loss: -0.025080] [G loss: -0.508185]\n",
            "[Epoch 48/50] [Batch 800/938] [D loss: -0.059281] [G loss: -0.473700]\n",
            "[Epoch 49/50] [Batch 0/938] [D loss: -0.078205] [G loss: -0.477845]\n",
            "[Epoch 49/50] [Batch 200/938] [D loss: -0.099243] [G loss: -0.466200]\n",
            "[Epoch 49/50] [Batch 400/938] [D loss: -0.066935] [G loss: -0.448346]\n",
            "[Epoch 49/50] [Batch 600/938] [D loss: -0.034113] [G loss: -0.465600]\n",
            "[Epoch 49/50] [Batch 800/938] [D loss: -0.056258] [G loss: -0.474823]\n",
            "[Epoch 50/50] [Batch 0/938] [D loss: -0.058667] [G loss: -0.484030]\n",
            "[Epoch 50/50] [Batch 200/938] [D loss: -0.074993] [G loss: -0.441493]\n",
            "[Epoch 50/50] [Batch 400/938] [D loss: -0.046494] [G loss: -0.449969]\n",
            "[Epoch 50/50] [Batch 600/938] [D loss: -0.071112] [G loss: -0.468124]\n",
            "[Epoch 50/50] [Batch 800/938] [D loss: -0.053077] [G loss: -0.473015]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVe_mrAIQr69"
      },
      "source": [
        "The best measurement of how good these GANs generate images is still our own eye sight. But there are also computable formulas such as the Inception Score (IS). The general idea behind the IS is to use a classifier to label a few generated images and then evaluate the variety of the outputs and how easily they could be classified. This blog post gives a simple explanation:\n",
        "\n",
        "https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a\n",
        "\n",
        "Read through the text and fill in the gaps in the code. First train the classifier and compute the Inception Score for both GAN models. Be aware that this will not be the real IS, since they use a different standardized classifier. For simplicity we are going to use this one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP1Y8KXGJQPv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "044ddff6-b4f6-49c7-cd82-d6fbb3ddcc9d"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "n_epochs = 8\n",
        "batch_size_train = 64\n",
        "lr = 0.001\n",
        "momentum = 0.9\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "class MNIST_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Classifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "classifier = MNIST_Classifier()\n",
        "classifier.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.cuda()\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "    accuracy = 0\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = classifier(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        accuracy += (outputs.argmax(dim = 1) == labels).float().mean().item()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i%200 == 199:\n",
        "            print(\n",
        "                  \"[Epoch %d/%d] [Batch %d/%d] [loss: %f] [accuracy: %f %%]\"\n",
        "                 % (epoch+1, n_epochs, i+1, len(dataloader), running_loss / 200, 100*accuracy/(i+1))\n",
        "            )\n",
        "            running_loss = 0.0\n",
        "       \n",
        "\n",
        "print('Finished Training')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1/8] [Batch 200/938] [loss: 2.301703] [accuracy: 10.203125 %]\n",
            "[Epoch 1/8] [Batch 400/938] [loss: 2.287652] [accuracy: 12.242188 %]\n",
            "[Epoch 1/8] [Batch 600/938] [loss: 2.252525] [accuracy: 21.156250 %]\n",
            "[Epoch 1/8] [Batch 800/938] [loss: 1.985538] [accuracy: 27.066406 %]\n",
            "[Epoch 2/8] [Batch 200/938] [loss: 0.774180] [accuracy: 75.609375 %]\n",
            "[Epoch 2/8] [Batch 400/938] [loss: 0.560885] [accuracy: 78.671875 %]\n",
            "[Epoch 2/8] [Batch 600/938] [loss: 0.425014] [accuracy: 81.351562 %]\n",
            "[Epoch 2/8] [Batch 800/938] [loss: 0.346156] [accuracy: 83.341797 %]\n",
            "[Epoch 3/8] [Batch 200/938] [loss: 0.236418] [accuracy: 92.867188 %]\n",
            "[Epoch 3/8] [Batch 400/938] [loss: 0.215511] [accuracy: 93.246094 %]\n",
            "[Epoch 3/8] [Batch 600/938] [loss: 0.193389] [accuracy: 93.523438 %]\n",
            "[Epoch 3/8] [Batch 800/938] [loss: 0.164612] [accuracy: 93.863281 %]\n",
            "[Epoch 4/8] [Batch 200/938] [loss: 0.142451] [accuracy: 95.773438 %]\n",
            "[Epoch 4/8] [Batch 400/938] [loss: 0.129199] [accuracy: 95.917969 %]\n",
            "[Epoch 4/8] [Batch 600/938] [loss: 0.132761] [accuracy: 95.934896 %]\n",
            "[Epoch 4/8] [Batch 800/938] [loss: 0.118252] [accuracy: 96.095703 %]\n",
            "[Epoch 5/8] [Batch 200/938] [loss: 0.106948] [accuracy: 96.726562 %]\n",
            "[Epoch 5/8] [Batch 400/938] [loss: 0.108976] [accuracy: 96.765625 %]\n",
            "[Epoch 5/8] [Batch 600/938] [loss: 0.103545] [accuracy: 96.815104 %]\n",
            "[Epoch 5/8] [Batch 800/938] [loss: 0.089027] [accuracy: 96.912109 %]\n",
            "[Epoch 6/8] [Batch 200/938] [loss: 0.093148] [accuracy: 97.171875 %]\n",
            "[Epoch 6/8] [Batch 400/938] [loss: 0.084510] [accuracy: 97.285156 %]\n",
            "[Epoch 6/8] [Batch 600/938] [loss: 0.083502] [accuracy: 97.348958 %]\n",
            "[Epoch 6/8] [Batch 800/938] [loss: 0.086176] [accuracy: 97.341797 %]\n",
            "[Epoch 7/8] [Batch 200/938] [loss: 0.080243] [accuracy: 97.593750 %]\n",
            "[Epoch 7/8] [Batch 400/938] [loss: 0.074989] [accuracy: 97.652344 %]\n",
            "[Epoch 7/8] [Batch 600/938] [loss: 0.075162] [accuracy: 97.645833 %]\n",
            "[Epoch 7/8] [Batch 800/938] [loss: 0.080333] [accuracy: 97.625000 %]\n",
            "[Epoch 8/8] [Batch 200/938] [loss: 0.068904] [accuracy: 97.843750 %]\n",
            "[Epoch 8/8] [Batch 400/938] [loss: 0.069976] [accuracy: 97.839844 %]\n",
            "[Epoch 8/8] [Batch 600/938] [loss: 0.068281] [accuracy: 97.854167 %]\n",
            "[Epoch 8/8] [Batch 800/938] [loss: 0.066022] [accuracy: 97.878906 %]\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D-ej3k8ZHyS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "547068e1-2248-40c6-ccaf-060f769fe021"
      },
      "source": [
        "from scipy.stats import entropy\n",
        "# you can use entropy(dist_1,dist_2) to compute KL-Divergence between\n",
        "# dist_1 and dist_2\n",
        "\n",
        "\n",
        "n_img = 100 # times batch_size\n",
        "\n",
        "preds_list = np.zeros((batch_size*n_img,10))\n",
        "\n",
        "def inceptionScore(generator):\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_img):\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (batch_size, latent_dim))))\n",
        "            gens = generator(z)\n",
        "            preds = F.softmax(classifier(gens), dim = 1)\n",
        "            preds_list[i*batch_size:(i+1)*batch_size]= preds.cpu()\n",
        "            \n",
        "    margin_dist = np.mean(preds_list, axis = 0)\n",
        "    \n",
        "    plt_pos = [i for i in range(10)]\n",
        "    print('The margin distribution:')\n",
        "    plt.bar(plt_pos, margin_dist, tick_label = plt_pos)\n",
        "    plt.show()\n",
        "    print('For a good model this should look nearly uniform.')\n",
        "    print('If that is not the case, you are experiencing a mode collapse')\n",
        "    print('which is a common problem for GANs')\n",
        "    \n",
        "    scores = []\n",
        "    for i in range(preds_list.shape[0]):\n",
        "        label_dist = preds_list[i,:]\n",
        "        scores.append(entropy(label_dist, margin_dist))\n",
        "    return np.exp(np.mean(scores))\n",
        "print('WGAN perfomance:')\n",
        "print(\"WGAN Inception Score: \", inceptionScore(w_generator))\n",
        "print('GAN perfomance:')\n",
        "print(\"GAN Inception Score: \", inceptionScore(generator))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WGAN perfomance:\n",
            "The margin distribution:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "For a good model this should look nearly uniform.\n",
            "If that is not the case, you are experiencing a mode collapse\n",
            "which is a common problem for GANs\n",
            "WGAN Inception Score:  6.75997191223102\n",
            "GAN perfomance:\n",
            "The margin distribution:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYLIrHRSBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "For a good model this should look nearly uniform.\n",
            "If that is not the case, you are experiencing a mode collapse\n",
            "which is a common problem for GANs\n",
            "GAN Inception Score:  1.1767441206159608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urxLdj5_Wj1D"
      },
      "source": [
        ""
      ]
    }
  ]
}