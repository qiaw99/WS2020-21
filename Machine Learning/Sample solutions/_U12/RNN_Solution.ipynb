{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "RNN_Solution.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "75VOiRumbtCW"
      },
      "source": [
        "# Text Generation with RNNs\n",
        "\n",
        "<small> credits to **Wael Amayri** (who was a teaching assistant together with me for the last class) </small>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "q-eZcyy_btCX"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Kwc0o4bGbtCa"
      },
      "source": [
        "### 1 Dataset\n",
        "Define the path of the file, you want to read and train the model on\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "_-pWsSdWbtCb"
      },
      "source": [
        "'''TODO: set the path of the file'''\n",
        "# Moby Dick text file from: https://gist.github.com/StevenClontz/4445774\n",
        "\n",
        "path_to_file = '/content/mobydick.txt'\n",
        "text = open(path_to_file, encoding='utf-8').read()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ku1gyuOibtCd"
      },
      "source": [
        "\n",
        "#### Inspect the dataset\n",
        "Take a look at the first 250 characters in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "hOoE3ZLwbtCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4712147e-f5ae-40d1-d674-d1f8d5d6d189"
      },
      "source": [
        "print(text[:250])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MOBY-DICK \n",
            "\n",
            "OR \n",
            "\n",
            "THE WHALE \n",
            "\n",
            "\n",
            "\n",
            "ETYMOLOGY \n",
            "\n",
            "(SUPPLIED BY A LATE CONSUMPTIVE USHER TO \n",
            "A GBAMMAB SCHOOL) \n",
            "\n",
            "THE pale Usher threadbare in coat, heart, body, and brain ; \n",
            "I see him now. He was ever dusting his old lexicons and \n",
            "grammars, with a queer han\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% \n",
          "is_executing": false
        },
        "id": "ixxZ4yBqbtCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43e61fa-4872-43be-86f1-52a955f178a0"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "luvcRu86btCj"
      },
      "source": [
        "### 2 Process the dataset for the learning task\n",
        "The task that we want our model to achieve is: given a character, or a sequence of characters, what is the most probable next character?\n",
        "\n",
        "To achieve this, we will input a sequence of characters to the model, and train the model to predict the output, that is, the following character at each time step. RNNs maintain an internal state that depends on previously seen elements, so information about all characters seen up until a given moment will be taken into account in generating the prediction.\n",
        "\n",
        "#### Vectorize the text\n",
        "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters. Recall that we just identified the unique characters present in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "7koc3Q2dbtCk"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Create a mapping from indices to characters\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "s04cWuBCbtCm"
      },
      "source": [
        "\n",
        "This gives us an integer representation for each character. Observe that the unique characters (i.e., our vocabulary) in the text are mapped as indices from 0 to len(unique). Let's take a peek at this numerical representation of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "7Rbt2hfpbtCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38aae9da-f0ac-466c-90e0-45d2210ae160"
      },
      "source": [
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(200)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  '$' :   4,\n",
            "  '&' :   5,\n",
            "  \"'\" :   6,\n",
            "  '(' :   7,\n",
            "  ')' :   8,\n",
            "  '*' :   9,\n",
            "  ',' :  10,\n",
            "  '-' :  11,\n",
            "  '.' :  12,\n",
            "  '/' :  13,\n",
            "  '0' :  14,\n",
            "  '1' :  15,\n",
            "  '2' :  16,\n",
            "  '3' :  17,\n",
            "  '4' :  18,\n",
            "  '5' :  19,\n",
            "  '6' :  20,\n",
            "  '7' :  21,\n",
            "  '8' :  22,\n",
            "  '9' :  23,\n",
            "  ':' :  24,\n",
            "  ';' :  25,\n",
            "  '<' :  26,\n",
            "  '>' :  27,\n",
            "  '?' :  28,\n",
            "  'A' :  29,\n",
            "  'B' :  30,\n",
            "  'C' :  31,\n",
            "  'D' :  32,\n",
            "  'E' :  33,\n",
            "  'F' :  34,\n",
            "  'G' :  35,\n",
            "  'H' :  36,\n",
            "  'I' :  37,\n",
            "  'J' :  38,\n",
            "  'K' :  39,\n",
            "  'L' :  40,\n",
            "  'M' :  41,\n",
            "  'N' :  42,\n",
            "  'O' :  43,\n",
            "  'P' :  44,\n",
            "  'Q' :  45,\n",
            "  'R' :  46,\n",
            "  'S' :  47,\n",
            "  'T' :  48,\n",
            "  'U' :  49,\n",
            "  'V' :  50,\n",
            "  'W' :  51,\n",
            "  'X' :  52,\n",
            "  'Y' :  53,\n",
            "  'Z' :  54,\n",
            "  '\\\\':  55,\n",
            "  '^' :  56,\n",
            "  '_' :  57,\n",
            "  'a' :  58,\n",
            "  'b' :  59,\n",
            "  'c' :  60,\n",
            "  'd' :  61,\n",
            "  'e' :  62,\n",
            "  'f' :  63,\n",
            "  'g' :  64,\n",
            "  'h' :  65,\n",
            "  'i' :  66,\n",
            "  'j' :  67,\n",
            "  'k' :  68,\n",
            "  'l' :  69,\n",
            "  'm' :  70,\n",
            "  'n' :  71,\n",
            "  'o' :  72,\n",
            "  'p' :  73,\n",
            "  'q' :  74,\n",
            "  'r' :  75,\n",
            "  's' :  76,\n",
            "  't' :  77,\n",
            "  'u' :  78,\n",
            "  'v' :  79,\n",
            "  'w' :  80,\n",
            "  'x' :  81,\n",
            "  'y' :  82,\n",
            "  'z' :  83,\n",
            "  '{' :  84,\n",
            "  '|' :  85,\n",
            "  '}' :  86,\n",
            "  '~' :  87,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Dsh270g9btCp"
      },
      "source": [
        "\n",
        "\n",
        "We can also look at how the first part of the text is mapped to an integer representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "egRTUKhlbtCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c208f65-890c-4f7e-d1e4-ab9ddb925256"
      },
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\nMOBY-DICK \\n\\n' ---- characters mapped to int ---- > [ 0 41 43 30 53 11 32 37 31 39  1  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fti3o5GHbtCs"
      },
      "source": [
        "#### Defining method to encode one hot labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "oef5BSBpbtCt"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RxoJlIhmbtCw"
      },
      "source": [
        "\n",
        "#### Defining method to make mini-batches for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "1JNdq5YgbtCw"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "\n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "\n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n + seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oHoy0u0abtCz"
      },
      "source": [
        "\n",
        "## 3 The Recurrent Neural Network (RNN) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Tkdc8Z6VbtC0"
      },
      "source": [
        "\n",
        "###### Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "5wRmMM9kbtC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec3bc62-ee0b-4b52-97c2-2204adf3f3f0"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "print ('Training on GPU' if train_on_gpu else 'Training on CPU')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "C4b7JO6FbtC3"
      },
      "source": [
        "\n",
        "### Declaring the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "k72_pm6obtC3"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab, n_hidden=256, n_layers=2,\n",
        "                 drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.vocab), n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        # define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.vocab))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network.\n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "\n",
        "        # get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        # put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hD6AFo9zbtC8"
      },
      "source": [
        "\n",
        "#### Declaring the train method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "S2HKSmmAbtC9"
      },
      "source": [
        "def train(model, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "\n",
        "        model: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    '''\n",
        "    model.train()\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if (train_on_gpu):\n",
        "        model.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_vocab = len(model.vocab)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = model.init_hidden(batch_size)\n",
        "        \n",
        "        '''TODO: use the get_batches function to generate sequences of the desired size'''\n",
        "        dataset = get_batches(data, batch_size, seq_length)\n",
        "\n",
        "        for x, y in dataset:\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_vocab)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if (train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = model(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = model.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                model.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_vocab)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if (train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = model(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                model.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                print(sample(model, 1000, prime='M', top_k=10))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wtlmj6sAbtC_"
      },
      "source": [
        "\n",
        "##### Defining a method to generate the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "eTDAUosNbtC_"
      },
      "source": [
        "def predict(model, char, h=None, top_k=None):\n",
        "    ''' Given a character, predict the next character.\n",
        "        Returns the predicted character and the hidden state.\n",
        "    '''\n",
        "\n",
        "    # tensor inputs\n",
        "    if char == 'و':\n",
        "      print(char)\n",
        "      return 0, h\n",
        "    x = np.array([[char2idx[char]]])\n",
        "    x = one_hot_encode(x, len(model.vocab))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if (train_on_gpu):\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = model(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if (train_on_gpu):\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(model.vocab))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return idx2char[char], h\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4a_I3GHqbtDB"
      },
      "source": [
        "\n",
        "#### Declaring a method to generate new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "mS9UcwBXbtDC"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "    if (train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    net.train()\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "viwhJM9RbtDE"
      },
      "source": [
        "\n",
        "#### Generate new Text using the RNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u7bc2y3DbtDE"
      },
      "source": [
        "###### Define and print the net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "mHbAsHB2btDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4324da15-387a-4211-9011-1b4212b23424"
      },
      "source": [
        "''''TODO: Try changing the number of units in the network to see how it affects performance'''\n",
        "n_hidden = 1024\n",
        "n_layers = 2\n",
        "\n",
        "model = CharRNN(vocab, n_hidden, n_layers)\n",
        "print(model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(88, 1024, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=88, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "e-sU3cQIbtDI"
      },
      "source": [
        "\n",
        "###### Declaring the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "ZJnyYkQlbtDJ"
      },
      "source": [
        "batch_size = 64\n",
        "seq_length = 10\n",
        "n_epochs = 10  # start smaller if you are just testing initial behavior"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "46LJ32XHbtDM"
      },
      "source": [
        "\n",
        "##### train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "rL0QLMXlbtDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337e84b0-f120-4893-e976-02b2b1b29a9a"
      },
      "source": [
        "'''TODO'''\n",
        "train(model, text_as_int, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=500)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 500... Loss: 1.6626... Val Loss: 1.5590\n",
            "Mink was so inridnest \n",
            "hunting to me ; he his pricies, but the sea of the shell cormer, the particulal sevoatian woll. \n",
            "W' ' The sumble of the fisher, and hus ofcemado arm on the ross, trunse from \n",
            "at march on our one present meaniar, whether the way, and \n",
            "seen him. I were has a farly of the whole,' said I was indougded wind, I see so it spope all horners ; hing or fret there had no blong. \n",
            "\n",
            "I Tertlidal fither bitce ; bet I an as a matire, whale ! \n",
            "\n",
            "\n",
            "\n",
            "CHAPTER VI. \n",
            "\n",
            "IL ! \n",
            "\n",
            "StHall a wasterouses, that at sea, and that where arm his owner, had shoulding seemed to hist the \n",
            "more crew. Starbuck, whose threasons shake there only idled by in origh-nore-ships all sort, and \n",
            "sillering wifd a maintre-corserance of \n",
            "dispas all our flongery but still,. \n",
            "\n",
            "BOOTOR. \n",
            "\n",
            "II these lass, ship's hast took throw this to mo,t tammucant, \n",
            "as wooding only \n",
            "all stleets white captain against a parmowing sinness \n",
            "as in the white whale went, when the fore fish, on \n",
            "supposed the whole crew. Thought that without with th\n",
            "Epoch: 2/10... Step: 1000... Loss: 1.6213... Val Loss: 1.5229\n",
            "Mord horns in \n",
            "tole to me on treakly been from \n",
            "has this harpoons.' \n",
            "\n",
            "4 That, a serving ouch feelingting me. To are, \n",
            "at the present must he than it that what was \n",
            "tashe of the \n",
            "pharter and award hand agriving many swarry's live \n",
            "wared with bold, \n",
            "hard the canent was spread. \n",
            "\n",
            "\n",
            "\n",
            "272 MOBY-DICK \n",
            "\n",
            "said the ship's paps-in telling from its, that whales he weste this whale's \n",
            "woods, who had bones with the bows, but the commor is in the prosable \n",
            "bold. Frank and witcreed black \n",
            "main and must bone with the \n",
            "contlence in the constitute the grand \n",
            "and spassing, and the menched to strange that the presented, \n",
            "to be deep all \n",
            "the mast, a boint so takingly twristed to \n",
            "this timble wengsted his cheekens, other stand, this mark till their thrifted their \n",
            "wholo, or a trick with a \n",
            "thander, to he bled at theich herronely the correction had but the \n",
            "close at larkel, off the barks, and here. In launging \n",
            "hearled and prace, on basin ; and it side of the staves his sea can \n",
            "was besting his soverbind of see a\n",
            "Epoch: 2/10... Step: 1500... Loss: 1.5787... Val Loss: 1.4984\n",
            "Martery had a \n",
            "crain of the subence \n",
            "from the placelones \n",
            "be flent the most mistary \n",
            "made by the low ; by the shorely \n",
            "promounsed. His south of the broad at \n",
            "the stovery swision, so for an ords of the \n",
            "cimies of the scips one in the slaetly replessual whiteness is that prained was so as mank, \n",
            "but intense- \n",
            "ingayed \n",
            "intulled, who \n",
            "had been abort, if the most anywhermed that I say. \n",
            "There, thus in these fore and the dearly \n",
            "tilineds, and spothed flamb, brimies ; if that \n",
            "captuined him, when all of what he had from that poships or ane ; from \n",
            "it, though are in the dece, when, he aspects \n",
            "to have have been bearse the turn \n",
            "of time of the stand beloshing threat soon \n",
            "about the back. And it is, bealed out of her but \n",
            "with a potent side of sout \n",
            "and and world-fane, much sean figu silent did tubling \n",
            "or bising stitting up his lanest. And when waithe ; brought \n",
            "is his particular in anstable wime, and would simply discessounds he 's not commedes to the whales, \n",
            "wuse holest twich of the wood still\n",
            "Epoch: 3/10... Step: 2000... Loss: 1.5192... Val Loss: 1.4832\n",
            "MOST-HY-WHOLE. \n",
            "\n",
            "Stop- \n",
            "stiped my lives trattest to must incereritg from the mare that sisting. Ot almost he ple- \n",
            "le to askent some \n",
            "almost making on the men are those bastly, off observed, and sometimes, \n",
            "though inselt to his captain Peleg and so hear ! ' cut tike him. The stutelish whale \n",
            "is all manty fishers, he centure of the sight woss ; it is a pet to it was to have go do by the ship \n",
            "from all the boat from myself ; acrooding up the matter of the thoughts \n",
            "beganest himself in one of the satardander into the head, bringing \n",
            "it. \n",
            "\n",
            "\n",
            "\n",
            "CHAPTER XVVIV \n",
            "\n",
            "THE TULN HOW HOBY-DICK \n",
            "\n",
            "we seen, that that handred we wrupp to have the hung, and then bottom of the whale, so, started out \n",
            "another hearty \n",
            "of that signations, almust here thank one swrongers of sharp \n",
            "it then to have seen on to be simble shouted \n",
            "him ; but her hold out of shipmates seemed touching \n",
            "the same and and poled tibking him, wife \n",
            "the misest, fine in the whale ; that wair- \n",
            "\n",
            "\n",
            "\n",
            "204 MOBY-DICK \n",
            "\n",
            "whalemen heading, and three same-b\n",
            "Epoch: 3/10... Step: 2500... Loss: 1.4738... Val Loss: 1.4605\n",
            "MOBY-DICK 289 \n",
            "\n",
            "indeed no tolarta it at the pipe's. \n",
            "\n",
            "' God specise that the ball and the way of peculations alore of \n",
            "demand of the \n",
            "\n",
            "VOL, \n",
            "I 've been that indifferable all the boss, to cash- \n",
            "most through when but that on a lrate, outward of the lead of the best way a \n",
            "dear at the morning, \n",
            "and then, were all indeed ; he 's the \n",
            "seas this winder which at still bench. Still must \n",
            "amough the please ; the possible commanded ifle- \n",
            "head were to mrelth of sport them aftic all the whale of that whale, \n",
            "and something of that deck, would not that ace, the thing with that stream, who, \n",
            "can the sea ; yes, take that alive the \n",
            "thing of the whole had assons to \n",
            "earth, indeed, madling it was a pilet of their wates, so told him who have \n",
            "been heisted thus spute in an alse. He harpoon, when you well know that harmooneer his \n",
            "planting the case to a monomshals. \n",
            "It was so smoker with me to an enoum, \n",
            "they steads. Fresh and whale has \n",
            "slippans we are other again in \n",
            "himself. \n",
            "\n",
            "'Tard that the morning at \n",
            "Epoch: 4/10... Step: 3000... Loss: 1.4417... Val Loss: 1.4567\n",
            "MANY-SUALE -BOBE AND FORECASTLEEDERAROS (\n",
            "THE QUARTEM-DECK 291 \n",
            "\n",
            "had him. But I could store to the leading towers that hence, \n",
            "thou generally boat those shudder of more curious feer feet agential. \n",
            "But I deper an amount in more things honour through \n",
            "the forecompare of your monshic as swimmers and all three table to. What were worndy \n",
            "heaved the thirding tylond arturely shootly saltar- \n",
            "sailors he concerned by, \n",
            "and all this clear must be shove \n",
            "my own provants to be calms without the feet. So such much of any tore. And to \n",
            "so carried the weapons callible almost things in this asture or twat there ! But thy compass this trin with \n",
            "the weal-body ; but where you chalters they touch our \n",
            "had been spots for a certain contrese, but hand to case moreing \n",
            "that the happing of them what with all the prespy serend \n",
            "shakings of a tamborroor wave that way in the stop, hustherow. I say ! what and \n",
            "\n",
            "The ballies. I told your tanners were his hory tended with him, for his mun far open., though, where I \n",
            "Epoch: 4/10... Step: 3500... Loss: 1.3716... Val Loss: 1.4412\n",
            "MATER \n",
            "\n",
            "THE Whale), sometimes, by the transper, hard shall our order of tell on it ! When a green \n",
            "mouthful corrict. I, where the same \n",
            "was of a light, this cannibal as he crossed before \n",
            "its accomparably \n",
            "tradining out a whale ; there 's \n",
            "toed the spirit will in me in his own. \n",
            "Tho superfact any particular sleather-ship attacked the hunt of whaler \n",
            "in the filted, and thouest the blead, than this \n",
            "been seen by his pats are beach, that \n",
            "the liner water and cheared and winder without sweet \n",
            "that take about thought to a stranger- \n",
            "wild noted or a boat ; to bill the believo it was chief that \n",
            "while suspecially taken a social oceans ones for the schet in \n",
            "the milent chresting humming all, touching \n",
            "to those frontic too sir, the pugcitionally called one \n",
            "is altuous instanity, and way \n",
            "it arise of his boat, and supmin of the secsed prin- \n",
            "twom of the fire horrid seas weld mittaliness senes, \n",
            "and wilting the headsteel of the stern world \n",
            "tastomed the sail in such an archine clack. \n",
            "And swim and \n",
            "Epoch: 5/10... Step: 4000... Loss: 1.4235... Val Loss: 1.4372\n",
            "Mory-Dick, that at this \n",
            "shere, when I had sleep true of ye. And there here is not \n",
            "and the pipe. and that in the middle on a man trate \n",
            "with a genalal inserted accompated the haupies of ship ; he darned and disconcedn back with the second \n",
            "costurate herds of the slances started at the boats. At interval ; \n",
            "that had sorcord. ' Cries ! That it was being \n",
            "sleeping-ground and closely chasing \n",
            "shoulders to hand to those \n",
            "answer of horrings, in made an interval ; and so then a tending \n",
            "so long as famous took \n",
            "for herdant seas from the way by \n",
            "the projocked packs of him ; the watch of some time with its sleeper-doubtlass. \n",
            "There were with horrous presured the lapple in the world's soul by the bast \n",
            "fhale. It \n",
            "seemed the soul in the weary of the whales. Wold the whole sea. Of hair \n",
            "that there you d you think then, had this whale, with \n",
            "crossed in only a while substantiated same was one of the captain's crown, with a harroons and \n",
            "more \n",
            "arasued at store of many large of the white \n",
            "world that six\n",
            "Epoch: 5/10... Step: 4500... Loss: 1.4128... Val Loss: 1.4343\n",
            "Mording \n",
            "hampes, but a counsing, as would crord, this is the censible and all incernible were \n",
            "for a line of my comrader or \n",
            "heavy wainster the head to the weather and creatures was attrocolity as the \n",
            "mate, we pleasantly asclubb and bowl ; and he \n",
            "have still feet for carses ; and in that ishander \n",
            "in our beach, ob woet- \n",
            "ing. \n",
            "\n",
            "' Here and that seeing off something much more through his head for all, selfon bursted up his morning and things on a waves. He \n",
            "contided a few men in many one of the same \n",
            "devil a hours at a cordude, strong its incantation ; \n",
            "off the person is thousand movements, toichled to my buttants \n",
            "seemed creatures was to him ; it 's hadd how as heathers of the chaping opening \n",
            "sleaving soul as a paper of this crew, and all \n",
            "inferitable to turned to the tleathen for sixe to be setting in the mirkiners, \n",
            "this was simple for them on his \n",
            "real coads of the world were subject. As the circumstantes \n",
            "of a salt sin with the study, on the stern \n",
            "\n",
            "HATTIGNS (Spill and heavy rescand\n",
            "Epoch: 6/10... Step: 5000... Loss: 1.2933... Val Loss: 1.4353\n",
            "Moby-Dick ! ' said I ; and the stewner \n",
            "are some once thing. A word time over water to the \n",
            "time of side with our ship, man is a firm \n",
            "are which as a ship.' \n",
            "\n",
            "' So, when a but on his mess was seen the officer of spiritual features of the sun, IIhma, why so lost \n",
            "what the one more frequent my concerded bitter of the more of my \n",
            "back of the fine at \n",
            "the stivery countenance ; bring his harpoon with that \n",
            "sea of that possible of all his hour through his \n",
            "dinest that we thought he was not more to his sheart of those work-pats of the deep by \n",
            "distrusting his were all remove as the sort ; this is that impreviously \n",
            "way or stifling in and at last strong \n",
            "arms, who does not stil, \n",
            "the whales, he was boats with their by an inch of the bales, \n",
            "which had heard to also deast. The whale -captain from his priscient conftious of the \n",
            "waters for stop, he considering and battle \n",
            "of darkness ashore all repeated. \n",
            "\n",
            "' Good by that thing, sadder things. But it some promasters the momongerous silent \n",
            "were as h\n",
            "Epoch: 7/10... Step: 5500... Loss: 1.2987... Val Loss: 1.4317\n",
            "Mortashes. By \n",
            "THE WHALE \n",
            "283 \n",
            "\n",
            "\n",
            "\n",
            "THE RAMADAN \n",
            "\n",
            "WHE ED NOUS MART 513 \n",
            "\n",
            "and made a sea-cret \n",
            "was some predictions of the sea and whaling, \n",
            "in the habitional soliniousness, and \n",
            "the principal pitch on the crew, and \n",
            "sloeps with the face and corrappress with a whale whe wrickles and slade. \n",
            "\n",
            "Now, at lamb ; he pived supposed the story that \n",
            "he come to the landlard consider, \n",
            "\n",
            "\n",
            "\n",
            "CHAPTER XXX \n",
            "\n",
            "THE AFFIDAVIT 255 \n",
            "\n",
            "with the forecastle inds looking \n",
            "him he is, in the ship's closed old story ; for is a fellow of all beard, \n",
            "and thly name of them to wore a face in the \n",
            "piecess, or last, he princed his bunished, while beds another sput, at her cross-turse. \n",
            "\n",
            "And seemed the first appearing against \n",
            "the based with indiritude to spay a pappring of the sea who was offerly could \n",
            "be, by some sort to the cool in those of the \n",
            "compased hold and most animal wooden treachery and subfrish it, it when what such importation of the land- \n",
            "less must have shitted between their happonited by a talkast ; and then \n",
            "s\n",
            "Epoch: 7/10... Step: 6000... Loss: 1.2983... Val Loss: 1.4269\n",
            "Mr.. SoUghot' Comfort, were some stated and saye to much \n",
            "a peculiar takes of the best, therefore, the \n",
            "dream- \n",
            "nusitys. \n",
            "\n",
            "The last. \n",
            "\n",
            "' Hall you '11 tell would threw of the wigwam arm of his sake \n",
            "above me in, but I would take the columal \n",
            "serrations ; and he is not far and brand the common born \n",
            "shock, and showing myself between the figure here ; and his clearness or hair ; through every \n",
            "truth-ended things has any of his sort of any or answer on announce of \n",
            "her might have too back as wropk about a cold, and this \n",
            "was and then \n",
            "bearterly remaintain. Standing for \n",
            "a sock, \n",
            "other manignity of a does on the larges. But \n",
            "it was those last thought a whale ; they \n",
            "could stand before the sperm whale has only breaking as any he seems his \n",
            "black captain had notened by \n",
            "the strangerrate with his machilitard \n",
            "to below, it agass the constantico fit \n",
            "again ; the strike answered bestowed upon the sails with a \n",
            "ferior of living \n",
            "sea through his file of hand, but impellatulty wise shade \n",
            "and subtleme\n",
            "Epoch: 8/10... Step: 6500... Loss: 1.2869... Val Loss: 1.4292\n",
            "Many \n",
            "come. Then ambing the conceit of the wharf, battled away ; his soil is only a smake bear to that sound, \n",
            "whether the twallows \n",
            "were now in his shality by all other sheep to me about the few large bench on the back and pole ; on. \n",
            "I never might be something might have his mast-head, wherefore \n",
            "these things, the superiority of with the profase we had toochant seeming up the whalo. \n",
            "\n",
            "But, as it was for my beard short, who like too, of the hunty ; wilous flee, haligudly consideration, that it is much \n",
            "forced there, with the heavy plank to him \n",
            "hy seemed the complete fin eye as if though seeing \n",
            "one thing to comfort, was not often might but for his face observed to \n",
            "the pain. But the sense of the lungs of deason ; yee, the constalting his \n",
            "noble treaches, and startly bindain why they are those stouter-deeping. \n",
            "Fedirtid his hand in the \n",
            "grimmanias \n",
            "this proports, and perilous dicamed ; sort of door, and \n",
            "broad sort has so sparing at him, and in all the spell way at last bruders to \n",
            "soun\n",
            "Epoch: 8/10... Step: 7000... Loss: 1.3006... Val Loss: 1.4209\n",
            "Mino of the moruity water \n",
            "had not so damping in order to a cold men ; feel \n",
            "\n",
            "\n",
            "\n",
            "THE AFFIDAVIT 259 \n",
            "\n",
            "almost so complete a profone too also then we \n",
            "blow from the deck ; the whale is smalls \n",
            "of the stern tatery, so a long time \n",
            "abourdered Starbuck 's colore, and then \n",
            "there have never been a milky of his \n",
            "best worthystells of the ship, \n",
            "what a cool, white mains indolently long suppose to stack \n",
            "that then before the landland in the \n",
            "place of the book of \n",
            "the Portical whaling house. And I \n",
            "stand a particular, \n",
            "is something was the long \n",
            "watch seat and heavens, one in the \n",
            "shell. \n",
            "\n",
            "Shipmates, and the mate, \n",
            "and the Whale ; then said I, \" the most in christian \n",
            "from \n",
            "then, by all much of his bugglide, that step and beach his forehead, and seemed \n",
            "norting the peculiar arm of the seas, all \n",
            "the cry lad again to foin't \n",
            "long. For, they are glancing for their \n",
            "corners's arrivide and monalted man. He is, if that moment of them, \n",
            "because they think. But I shall help a pity, and \n",
            "answer a settled on \n",
            "Epoch: 9/10... Step: 7500... Loss: 1.2941... Val Loss: 1.4275\n",
            "MOBY-DICK \n",
            "\n",
            "what I had been all \n",
            "discovering the tasked manner ; not \n",
            "somewhat stiff waters of \n",
            "his ere thick than his commotion that in port of the white \n",
            "forks borrowed but nothing abundantly and day against that possision \n",
            "of his peremption who called, \n",
            "shaking about wearied mate of its archibilate to stop which would take \n",
            "a fish, wholly to that will cetilino \n",
            "to see him for it for the forehard of him, but when I thought \n",
            "a butterflies ; there the \n",
            "seven-fold sort of thing instant an old men \n",
            "has the first pats for the weather ; \n",
            "and even as the mystic whale is made the same side of warm \n",
            "of the purpose ; therefore \n",
            "that he said he have \n",
            "been serted the watery who says, I said he, shore in my \n",
            "own coupon of six, in some day-bried, that the more could be a rather \n",
            "chief peculiarities of whales, and standing into his honest miscivition, and how it struck himself ; \n",
            "out of them without \n",
            "another crewal samothom of the \n",
            "dead, they harpooneer to commen \n",
            "have had to torrie with thought ; so\n",
            "Epoch: 9/10... Step: 8000... Loss: 1.1775... Val Loss: 1.4267\n",
            "MaN ' (\n",
            "SACRED 159 \n",
            "\n",
            "and then a small \n",
            "peep the white chief man those self-crowded \n",
            "crew, he tritting one of them at the wind, which places we have, as for my other sheepers ; but \n",
            "those more complied is hard in all \n",
            "three isses of \n",
            "that cannot prophetic people \n",
            "cried thy more contribut wonding the ship's company thick, for the \n",
            "however. As for how it was the bread \n",
            "stopies and pounded thing with his one and desperace \n",
            "former tweary's love was catching him \n",
            "into the blund. \n",
            "\n",
            "For something an inlittle and wearen-handed horizontal, was the stranger than that sight of what \n",
            "is a giorate ancount. \n",
            "\n",
            "But the cry. \n",
            "\n",
            "4 Chare masts had been artide and assured, in the cherish \n",
            "than account to them to be been hi magnetures by the helm, haiding \n",
            "a ships me ; his then circumstance \n",
            "in a sharp at the first pleasant twine in the limber wosks of him is \n",
            "a sailor, and to those was travelled to pound \n",
            "through that hope in the sea, from the spout what there be it, \n",
            "when something heard it speaking his lows\n",
            "Epoch: 10/10... Step: 8500... Loss: 1.2808... Val Loss: 1.4282\n",
            "Morusa at long. Stab- \n",
            "lord than once sounds with many way, as shouse, that to choma trunk's \n",
            "captain, and the white world blended on the time without the best thing and terrors \n",
            "cities, and went for any morely small in their cunstion. They \n",
            "demanded out of \n",
            "it in that white trousest hands, that hard her whole brow, and \n",
            "needed no more than in their last, too, the prodigy of the bulwarks of their sorts. But at last, where the \n",
            "damp, hopeless, to about some way and resulting. \n",
            "\n",
            "\n",
            "\n",
            "CHAPTER XV \n",
            "\n",
            "ALBASTSCHIS TS \n",
            "\n",
            "WHALE articled \n",
            "aspect. ' If \n",
            "hards were now sail a story outstone, motive assible a life- \n",
            "whole brow, and that they have an imagination with the whole old ferionious phanter- \n",
            "ing an iper of an untoly official instances as a listrish to his present \n",
            "and mosch before the store of the spare. \n",
            "\n",
            "' Bvoke no ; true eximerifor, that a silence of the berch in talling, \n",
            "had threw at leesing story to all commsted and lassing his forehead. \n",
            "What it is often mended the sayes, \n",
            "them became thro\n",
            "Epoch: 10/10... Step: 9000... Loss: 1.2578... Val Loss: 1.4377\n",
            "Meantime, of all its atraces \n",
            "of way of the sperm whale. There 's no touching out of the \n",
            "man, but is completely and radely hailing his bows \n",
            "a really releaped. There, \n",
            "sweeted from me ! \" said he, at large amazement ; because my first place, I see with it \n",
            "on to this shock high him whether had \n",
            "treat, his hands off. Fratting hand ; for all the tablets state, and ofter bleak into the part of some \n",
            "tambourine, \n",
            "and it were a whale, but waking \n",
            "and wanted it, and as much \n",
            "me ; and from the clear conormons and at all abovate \n",
            "means are places were corrected him to be found out by the sleet, \n",
            "before the forecastle course or \n",
            "those settled and too these contrastings ; a ship over and times and but \n",
            "him with \n",
            "those outer harpooneer, with the crew was to be serned to be done has he to him his species of the sea is this predections of these men were selmost \n",
            "angirscorple in the prairiets which might have been saying the \n",
            "truncher's boats by a helk, \n",
            "he sat seemed as the masts but self cries in a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECc7lKVcS2KZ"
      },
      "source": [
        ""
      ]
    }
  ]
}